{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-boost",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "自动微分 torch.autograd;\n",
    "\n",
    "在训练神经网络时，最常用的算法是 反向传播。在该算法中，根据损失函数相对于给定参数的梯度来调整参数（模型权重）。\n",
    "\n",
    "PyTorch具有一个称为的内置微分引擎torch.autograd。它支持任何计算图的梯度自动计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "british-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([1., 1., 1., 1., 1.])\n",
      "y:  tensor([0., 0., 0.])\n",
      "w:  tensor([[-1.1373,  1.5912, -0.0512],\n",
      "        [-0.5054, -0.9429,  0.0996],\n",
      "        [ 1.0302,  0.4610, -0.7176],\n",
      "        [-1.4975,  0.0401,  1.0779],\n",
      "        [ 0.9736, -0.4620, -0.0244]], requires_grad=True)\n",
      "b:  tensor([ 0.6149,  0.9079, -0.9704], requires_grad=True)\n",
      "z:  tensor([-0.5214,  1.5953, -0.5861], grad_fn=<AddBackward0>)\n",
      "loss:  tensor(0.8962, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 最简单的单层神经网络，它具有input x，parametersw和b，并且具有一些损失函数。\n",
    "# 可以通过以下方式在PyTorch中定义：\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "print(\"x: \", x)\n",
    "y = torch.zeros(3) # exported output\n",
    "print(\"y: \", y)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "print(\"w: \", w)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "print(\"b: \", b)\n",
    "z = torch.matmul(x, w) + b\n",
    "print(\"z: \", z)\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-missouri",
   "metadata": {},
   "source": [
    "#### Tensors, Functions and Computational graph\n",
    "张量，函数和计算图\n",
    "\n",
    "w并b为参数，这是我们需要优化。因此，我们需要能够针对这些变量计算损失函数的梯度。\n",
    "\n",
    "为了做到这一点，我们设置了requires_grad这些张量的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunset-access",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f8c8ac1c150>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f8c8abbdb10>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-deadline",
   "metadata": {},
   "source": [
    "#### Computing Gradients\n",
    "计算梯度\n",
    "\n",
    "要计算损失函数相对于参数的导数，即，我们需要 ∂loss/∂w 和 ∂loss/∂bx和的一些固定值下 y。\n",
    "\n",
    "要计算这些导数，我们调用 loss.backward()，然后从w.grad和 检索值b.grad："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "close-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1242, 0.2771, 0.1192],\n",
      "        [0.1242, 0.2771, 0.1192],\n",
      "        [0.1242, 0.2771, 0.1192],\n",
      "        [0.1242, 0.2771, 0.1192],\n",
      "        [0.1242, 0.2771, 0.1192]])\n",
      "tensor([0.1242, 0.2771, 0.1192])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-error",
   "metadata": {},
   "source": [
    "#### Disabling Gradient Tracking\n",
    "不使用梯度跟踪。在推理时候，只想对于输入数据通过网络进行正向计算。使用torch.no_grad()停止梯度计算。\n",
    "\n",
    "禁用梯度跟踪的原因有以下几种：\n",
    "\n",
    "1、将神经网络中的某些参数标记为冻结参数。这是微调预训练网络的非常常见的情况\n",
    "\n",
    "2、为了加快运算速度，当你只是做向前传球，因为张量计算不跟踪梯度会更有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demonstrated-window",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monetary-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 另一种方法也可以：deatch()\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-experiment",
   "metadata": {},
   "source": [
    "#### More on Computational Graphs\n",
    "autograd在由功能 对象组成的有向无环图（DAG）中记录数据（张量）和所有已执行的操作（以及由此产生的新张量） 。在此DAG中，叶子是输入张量，根是输出张量。通过从根到叶跟踪该图，可以使用链规则自动计算梯度。\n",
    "\n",
    "在前向传递中，autograd同时执行两项操作：\n",
    "\n",
    "（1）运行请求的操作以计算结果张量\n",
    "（2）维护DAG中操作的梯度函数。\n",
    "\n",
    ".backward()在DAG根目录上调用时，向后传递开始。autograd然后：\n",
    "\n",
    "（1）从每个计算梯度.grad_fn，\n",
    "（2）将它们累积在各自的张量.grad属性中\n",
    "（3）使用链规则，一直传播到叶张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-event",
   "metadata": {},
   "source": [
    "#### Tensor Gradients and Jacobian Product\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
